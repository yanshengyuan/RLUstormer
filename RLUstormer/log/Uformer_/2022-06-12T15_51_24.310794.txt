Namespace(arch='Uformer', att_se=False, batch_size=16, checkpoint=50, dataset='SIDD', embed_dim=16, env='_', eval_workers=2, global_skip=False, gpu='0,1,2,3,4,5,6,7', local_skip=False, lr_initial=0.0002, mode='denoising', nepoch=250, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='./log/Uformer32/models/model_best.pth', resume=False, save_dir='/home/ma-user/work/deNoTr/log', save_images=False, token_embed='linear', token_mlp='leff', train_dir='../GoPro/train', train_ps=128, train_workers=4, val_dir='../GoPro/test', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=True, warmup_epochs=3, weight_decay=0.02, win_size=8)
Uformer(
  embed_dim=16, token_projection=linear, token_mlp=leff,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (feature_fus_0): FeatureFus(
    (conv): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=16, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=1
          (qkv): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
          (project_out): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (project_out): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=1
          (qkv): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
          (project_out): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (project_out): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (feature_fus_1): FeatureFus(
    (conv): Sequential(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=2
          (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (feature_fus_2): FeatureFus(
    (conv): Sequential(
      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=4
          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (feature_fus_3): FeatureFus(
    (conv): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=8
          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=256, input_resolution=(8, 8), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          (project_out): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=16
          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
          (project_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          (project_out): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=128, input_resolution=(16, 16), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=16
          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=16
          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
          (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
          (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=8
          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=8
          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=32, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=4
          (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=4
          (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
          (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=16, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=2
          (qkv): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
          (project_out): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (project_out): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): LeWinTransformerBlock(
        dim=16, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=16, win_size=(8, 8), num_heads=2
          (qkv): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (qkv_dwconv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
          (project_out): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=16, out_features=16, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (mlp): LeFF(
          (project_in): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
          (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (project_out): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
)
[Ep 1 it 32	 PSNR SIDD: 25.3852	] ----  [best_Ep_SIDD 1 best_it_SIDD 32 Best_PSNR_SIDD 25.3852] 
[Ep 1 it 65	 PSNR SIDD: 25.5547	] ----  [best_Ep_SIDD 1 best_it_SIDD 65 Best_PSNR_SIDD 25.5547] 
[Ep 1 it 98	 PSNR SIDD: 25.5816	] ----  [best_Ep_SIDD 1 best_it_SIDD 98 Best_PSNR_SIDD 25.5816] 
[Ep 1 it 131	 PSNR SIDD: 25.5879	] ----  [best_Ep_SIDD 1 best_it_SIDD 131 Best_PSNR_SIDD 25.5879] 
Epoch: 1	Time: 465.3561	Loss: 4.9455	LearningRate 0.000133
[Ep 2 it 32	 PSNR SIDD: 25.5994	] ----  [best_Ep_SIDD 2 best_it_SIDD 32 Best_PSNR_SIDD 25.5994] 
[Ep 2 it 65	 PSNR SIDD: 25.6089	] ----  [best_Ep_SIDD 2 best_it_SIDD 65 Best_PSNR_SIDD 25.6089] 
[Ep 2 it 98	 PSNR SIDD: 25.6196	] ----  [best_Ep_SIDD 2 best_it_SIDD 98 Best_PSNR_SIDD 25.6196] 
[Ep 2 it 131	 PSNR SIDD: 25.6217	] ----  [best_Ep_SIDD 2 best_it_SIDD 131 Best_PSNR_SIDD 25.6217] 
Epoch: 2	Time: 436.5213	Loss: 4.3779	LearningRate 0.000200
[Ep 3 it 32	 PSNR SIDD: 25.6298	] ----  [best_Ep_SIDD 3 best_it_SIDD 32 Best_PSNR_SIDD 25.6298] 
[Ep 3 it 65	 PSNR SIDD: 25.6268	] ----  [best_Ep_SIDD 3 best_it_SIDD 32 Best_PSNR_SIDD 25.6298] 
[Ep 3 it 98	 PSNR SIDD: 25.6379	] ----  [best_Ep_SIDD 3 best_it_SIDD 98 Best_PSNR_SIDD 25.6379] 
[Ep 3 it 131	 PSNR SIDD: 25.6382	] ----  [best_Ep_SIDD 3 best_it_SIDD 131 Best_PSNR_SIDD 25.6382] 
Epoch: 3	Time: 427.1524	Loss: 4.3211	LearningRate 0.000200
[Ep 4 it 32	 PSNR SIDD: 25.6417	] ----  [best_Ep_SIDD 4 best_it_SIDD 32 Best_PSNR_SIDD 25.6417] 
[Ep 4 it 65	 PSNR SIDD: 25.6425	] ----  [best_Ep_SIDD 4 best_it_SIDD 65 Best_PSNR_SIDD 25.6425] 
[Ep 4 it 98	 PSNR SIDD: 25.6393	] ----  [best_Ep_SIDD 4 best_it_SIDD 65 Best_PSNR_SIDD 25.6425] 
[Ep 4 it 131	 PSNR SIDD: 25.6429	] ----  [best_Ep_SIDD 4 best_it_SIDD 131 Best_PSNR_SIDD 25.6429] 
Epoch: 4	Time: 424.1446	Loss: 4.2394	LearningRate 0.000200
[Ep 5 it 32	 PSNR SIDD: 25.6438	] ----  [best_Ep_SIDD 5 best_it_SIDD 32 Best_PSNR_SIDD 25.6438] 
[Ep 5 it 65	 PSNR SIDD: 25.6441	] ----  [best_Ep_SIDD 5 best_it_SIDD 65 Best_PSNR_SIDD 25.6441] 
[Ep 5 it 98	 PSNR SIDD: 25.6413	] ----  [best_Ep_SIDD 5 best_it_SIDD 65 Best_PSNR_SIDD 25.6441] 
[Ep 5 it 131	 PSNR SIDD: 25.6426	] ----  [best_Ep_SIDD 5 best_it_SIDD 65 Best_PSNR_SIDD 25.6441] 
Epoch: 5	Time: 440.6484	Loss: 4.2623	LearningRate 0.000200
[Ep 6 it 32	 PSNR SIDD: 25.6420	] ----  [best_Ep_SIDD 5 best_it_SIDD 65 Best_PSNR_SIDD 25.6441] 
[Ep 6 it 65	 PSNR SIDD: 25.6445	] ----  [best_Ep_SIDD 6 best_it_SIDD 65 Best_PSNR_SIDD 25.6445] 
[Ep 6 it 98	 PSNR SIDD: 25.6428	] ----  [best_Ep_SIDD 6 best_it_SIDD 65 Best_PSNR_SIDD 25.6445] 
